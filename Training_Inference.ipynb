{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries"
      ],
      "metadata": {
        "id": "Ve7b_ovFXJh2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpOrbLiFWrpK"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install sentencepiece sacremoses\n",
        "!pip install tensorflow\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing packages"
      ],
      "metadata": {
        "id": "Lf1BeSOfXRZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "from torch.hub import load"
      ],
      "metadata": {
        "id": "woWu45nJWsn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loadind Base Model and Data"
      ],
      "metadata": {
        "id": "TJm1SI3XXVqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased').to('cuda')\n",
        "tokenizer = load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('path_to_data.csv')\n",
        "texts = data['data_noPause'].values.tolist()\n",
        "old_labels = data['category'].tolist()\n",
        "\n",
        "# Tokenization\n",
        "# tokens = [tokenizer.encode(text, max_length=256, pad_to_max_length=True) for text in texts]\n",
        "tokens = [tokenizer.encode(\" \".join(map(str, text)), max_length=256, padding=\"max_length\", truncation=True) for text in texts]\n",
        "labels = [0 if item == 'Control' else 1 if item == 'dementia' else item for item in old_labels]\n",
        "num_labels = 1\n",
        "reconstruct_embedding = 264 # 256+8\n",
        "# Convert to tensors\n",
        "input_ids = torch.tensor(tokens)\n",
        "attention_mask = (input_ids != 0).float()\n",
        "pause_count= torch.tensor(data[['pause1','pause2','pause3_noLoPause']].values.tolist())\n",
        "labels = torch.tensor(labels)\n",
        "labels = labels.to(dtype=torch.long)\n",
        "\n",
        "\n",
        "# Specify the number of folds\n",
        "num_folds = 20\n",
        "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store results for each fold\n",
        "f1_scores = []\n",
        "accuracies = []"
      ],
      "metadata": {
        "id": "E779tYs5Wws9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing The model Artchitecture"
      ],
      "metadata": {
        "id": "ifdWsXA-XbId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_all_weights(model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    refs:\n",
        "        - https://discuss.pytorch.org/t/how-to-re-set-alll-parameters-in-a-network/20819/6\n",
        "        - https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch\n",
        "        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    \"\"\"\n",
        "    @torch.no_grad()\n",
        "    def weight_reset(m: nn.Module):\n",
        "        # - check if the current module has reset_parameters & if it's callabed called it on m\n",
        "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
        "        if callable(reset_parameters):\n",
        "            m.reset_parameters()\n",
        "    # Applies fn recursively to every submodule see: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    model.apply(fn=weight_reset)\n",
        "\n",
        "# Model\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_labels):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.linear1 = nn.Linear(768, 256) #new\n",
        "        self.linear2 = nn.Linear(256, num_labels)  # 768 is BERT's output size\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "        drop_output = self.dropout(pooled_output)\n",
        "        embbeding = self.linear1(drop_output)\n",
        "        out = self.linear2(embbeding)\n",
        "        return out, embbeding\n",
        "\n",
        "\n",
        "class EncoderClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(EncoderClassifier, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),  # Adjust the number of hidden units as needed\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 8)  # Bottleneck layer with dimensionality 1 ,16,8,\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 256),  # Adjust the number of hidden units as needed\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded,decoded\n",
        "\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, bert_model, autoencoder, num_labels, reconstruct_embedding):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.autoencoder = autoencoder\n",
        "        self.fusion = nn.Linear(reconstruct_embedding , num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,pause_count):\n",
        "        outputs, embbeding = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Encode input data using the autoencoder\n",
        "        encoded,encoded_pause = self.autoencoder(pause_count)\n",
        "        # Concatenate the BERT outputs and encoded data\n",
        "        concat = torch.cat([embbeding, encoded], dim=1)\n",
        "        # print(\"embbeding\",embbeding.shape)\n",
        "        # print(\"encoded\",encoded.shape)\n",
        "        # Perform classification or regression on the concatenated data\n",
        "        out = self.fusion(concat)\n",
        "        return out, embbeding, encoded_pause\n",
        "\n",
        "def test(classifier,val_dataloader):\n",
        "    classifier.eval()\n",
        "    all_labels1 = []\n",
        "    all_predictions1 = []\n",
        "    threshold = 0.5\n",
        "    for input_ids_val, attention_mask_val,pause_count_val, labels_val in val_dataloader:\n",
        "        input_ids_val, attention_mask_val, pause_count_val, labels_val = input_ids_val.to('cuda'), attention_mask_val.to('cuda'), pause_count_val.float().to('cuda'),labels_val.float().unsqueeze(1).to('cuda')\n",
        "        with torch.no_grad():\n",
        "            outputs,_,_= classifier(input_ids_val, attention_mask_val,pause_count_val)\n",
        "\n",
        "        predictions =  (outputs.squeeze() > threshold).float().unsqueeze(1)\n",
        "        # Append labels and predictions to the lists\n",
        "        all_labels1.extend(labels_val.cpu().numpy())\n",
        "        all_predictions1.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_labels1 = np.array(all_labels1)\n",
        "    all_predictions1 = np.array(all_predictions1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels1, all_predictions1)\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(all_labels1, all_predictions1)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(all_labels1, all_predictions1)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(all_labels1, all_predictions1)\n",
        "\n",
        "#     print(f'Accuracy: {accuracy:.4f}')\n",
        "#     print(f'Precision: {precision:.4f}')\n",
        "#     print(f'Recall: {recall:.4f}')\n",
        "#     print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "    return f1,precision,recall,accuracy"
      ],
      "metadata": {
        "id": "yC9Ku41JXBOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training And Inference"
      ],
      "metadata": {
        "id": "NIbhLelUXnX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertClassifier(bert, num_labels).to('cuda')\n",
        "encoder_classifier = EncoderClassifier(input_dim=3).to('cuda')\n",
        "classifier = Classifier(bert_model, encoder_classifier, num_labels,reconstruct_embedding).to('cuda')\n",
        "#save classifier model and used it for each fold\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-5)\n",
        "mse = nn.MSELoss()\n",
        "loss_fn = nn.BCEWithLogitsLoss() # contains sigmoid activation for binary classification\n",
        "path_initial = '/content/drive/MyDrive/Pause_Encoding/path_initial'\n",
        "torch.save(classifier.state_dict(), path_initial)\n",
        "print('len(input_ids)',len(input_ids))\n",
        "# Training loop\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(input_ids)), labels.cpu().numpy())):\n",
        "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "    classifier.load_state_dict(torch.load(path_initial))\n",
        "    classifier.train()\n",
        "\n",
        "    # Initialization for best model path within the fold loop\n",
        "    best_acc = 0\n",
        "    lam = 0.75\n",
        "    path_best_model = None  # Initialize to None\n",
        "    # Initialize lists for each fold\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    #get the train set and val set\n",
        "    X_train, y_train = input_ids[train_idx].cpu().numpy(), labels[train_idx].cpu().numpy()\n",
        "    X_val, y_val = input_ids[val_idx].cpu().numpy(), labels[val_idx].cpu().numpy()\n",
        "\n",
        "    pause_count_train = np.array(pause_count[train_idx])\n",
        "    attention_mask_train = np.array(attention_mask[train_idx])\n",
        "    pause_count_val = np.array(pause_count[val_idx])\n",
        "    attention_mask_val = np.array(attention_mask[val_idx])\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    input_ids_train, attention_mask_train, pause_count_train, labels_train = (\n",
        "        torch.tensor(X_train),\n",
        "        torch.tensor(attention_mask_train),\n",
        "        torch.tensor(pause_count_train),\n",
        "        torch.tensor(y_train),\n",
        "    )\n",
        "    input_ids_val, attention_mask_val, pause_count_val, labels_val = (\n",
        "        torch.tensor(X_val),\n",
        "        torch.tensor(attention_mask_val),\n",
        "        torch.tensor(pause_count_val),\n",
        "        torch.tensor(y_val),\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_data = TensorDataset(input_ids_train, attention_mask_train, pause_count_train, labels_train)\n",
        "    val_data = TensorDataset(input_ids_val, attention_mask_val, pause_count_val, labels_val)\n",
        "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=8)\n",
        "\n",
        "    # Training loop for the current fold\n",
        "    for epoch in range(10):\n",
        "        classifier.train()\n",
        "        current_path_best_model = f'/content/drive/MyDrive/Pause_Encoding/best_model_fold_{fold}.pth'\n",
        "        for input_ids_train, attention_mask_train, pause_count_train, labels_train in train_dataloader:\n",
        "            input_ids_train, attention_mask_train, pause_count_train, labels_train = (\n",
        "                input_ids_train.to('cuda'),\n",
        "                attention_mask_train.to('cuda'),\n",
        "                pause_count_train.float().to('cuda'),\n",
        "                labels_train.float().unsqueeze(1).to('cuda'),\n",
        "            )\n",
        "            outputs, emb, encoded_pause = classifier(input_ids_train, attention_mask_train, pause_count_train)\n",
        "            loss_cross = loss_fn(outputs, labels_train)\n",
        "            reg_loss = mse(encoded_pause, pause_count_train)\n",
        "            loss = loss_cross + lam * reg_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        test_acc,_,_,_ = test(classifier, val_dataloader)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            path_best_model = current_path_best_model\n",
        "            torch.save(classifier.state_dict(), path_best_model)\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    if path_best_model is not None:\n",
        "        classifier.load_state_dict(torch.load(path_best_model))\n",
        "    else:\n",
        "        print(\"No best model found. Training may not have improved accuracy.\")\n",
        "    print(f\"Fold {fold + 1} Report\")\n",
        "    f11,precision1,recall1,accuracy1 =test(classifier, val_dataloader)\n",
        "    print(f'Accuracy: {accuracy1:.4f}')\n",
        "    print(f'Precision: {precision1:.4f}')\n",
        "    print(f'Recall: {recall1:.4f}')\n",
        "    print(f'F1 Score: {f11:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Fold {fold + 1} Report\")\n",
        "\n",
        "\n",
        "    f1_scores.append(f11)\n",
        "    accuracies.append(accuracy1)\n",
        "    # reset_all_weights(classifier)\n",
        "# Calculate mean and standard deviation\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_dev_f1 = np.std(f1_scores)\n",
        "\n",
        "mean_acc = np.mean(accuracies)\n",
        "std_dev_acc = np.std(accuracies)\n",
        "\n",
        "print(f'Mean F1 Score: {mean_f1:.4f}, Std. Dev. F1 Score: {std_dev_f1:.4f}')\n",
        "print(f'Mean Accuracy: {mean_acc:.4f}, Std. Dev. Accuracy: {std_dev_acc:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xJuBV1SUXGLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}